\chapter{One Dimensional Stochastic Case}\label{chap:oned-stochastic}

Here we return to the one dimensional version of this equation, but this time
we introduce some form of uncertainty in the coefficients of the equation:

\begin{equation}\label{eq:oned-stochastic}
  \begin{aligned}
      -\frac{d}{dx}\left[a(x;\omega)\frac{d}{dx}u(x;\omega)\right] &= f(x)\,
                              &x \in  D = [-1,1]\, \omega \in \Omega \\
    u(x;\omega) &= 0 &x \in \partial D = \{-1,1\}\, \omega \in \Omega
  \end{aligned}
\end{equation}

where $\Omega$ is a probability space. In order to handle this case, we follow
a similar method to that we used in Chapter \ref{chap:oned-deterministic}
however with the introduction of the random parameter $\omega$ there is an
additional layer of complexity to deal with, as well as discretising the
physical domain we also need to be able to discretise the probability space
using a finite dimensional approximation.  This is where the generalised
polynomial chaos comes in.

Using the methodology outlined in \cite{general-poly-chaos} we can use various
polynomials in the \textit{Askey Scheme} to construct a finite dimensional
approximate representations of various random processes which when used in
conjunction with the Finite Element Method we can model the expected behaviour
of the solution process $u(x;\omega)$

\section{Weak Formulation}

As with the deterministic cases, we first need to obtain the weak formulation
of the problem, which involves taking the inner product of
\myref{eq:oned-stochastic} with a function $w$ from a \textit{test space} $W$.
With the weak solution being a function $u$ from a \textit{trial space} $V$
which satisfies the resulting equation $\forall w \in W$.

In this case our function spaces not only have to admit functions on the
physical domain but on the probability spaces also, hence the trial and test
spaces are given by:

\begin{equation}
    \begin{array}{c c}
        V = H^1_0(D) \otimes L^2(\Omega) &
        W = H^1_0(D) \otimes L^2(\Omega)
    \end{array}
\end{equation}

Just as in Chapter \ref{chap:oned-deterministic} $H^1_0(D)$ refers to the
closure of $C_0^\infty(D)$ with respect to the Sobolev space norm
\myref{eq:oned-H1-D-norm} and $L^2(\Omega)$ is as defined in Definition
\ref{def:L2-Omega}. Multiplying through by $w \in W$ and integrating over
$\Omega \times D$ we obtain:

\begin{equation}
    -\int_{\Omega}\int_{-1}^1\left(\frac{d}{dx}\left[a(x;\omega)\frac{d}{dx}u(x;\omega)\right]
    w(x;\omega)\right)\, dx\, d\Omega = \int_{\Omega}\int_{-1}^1 f(x)w(x;\omega)\, dx\, d\Omega
\end{equation}

Proceeding with integration by parts

\begin{equation}
    -\int_{\Omega}\left(
      \underbrace{\left[a(x;\omega)\frac{d}{dx}u(x;\omega)
                        w(x;\omega)\right]_{-1}^1}_{= 0}
       -\int_{-1}^1a(x;\omega)\frac{d}{dx}u(x;\omega)\frac{d}{dx}w(x;\omega)\, dx
    \right)\, d\Omega = \int_{\Omega}\int_{-1}^1 f(x)w(x;\omega)\, dx\, d\Omega
\end{equation}

where the underbraced term is zero as the elements of $W$ are zero at the
endpoints of the domain. Hence the weak form of this equation is given by:

\begin{equation}\label{eq:wk-one-d-stochastic}
    \int_{\Omega}\int_{-1}^1a(x;\omega)
           \frac{d}{dx}u(x;\omega)\frac{d}{dx}w(x;\omega)\, dx\, d\Omega =
           \int_{\Omega}\int_{-1}^1 f(x)w(x;\omega)\, dx\, d\Omega
\end{equation}

where finding a solution in this sense would be to find a $u \in V$ such that
the above is satisfied $\forall w \in W$

\section{Discrete Formulation}

Constructing a finite dimensional approximation to
\myref{eq:wk-one-d-stochastic} requires us to discretise both physical and
probability spaces. For the physical space we can proceed in a similar fashion
to Chapter \ref{chap:oned-deterministic} taking into account that the domain
now extends to include the interval $[-1, 0]$

Given a parameter $N \in \mathbb{N}$ we place $N+1$ equally spaced nodes
$x_i, i \in \{0,\ldots,N\}$ in the interval $[-1,1]$ such that $x_0= -1$,
$x_N = 1$. This subdivides the interval into $N$ subintervals
$I_i = [x_i, x_{i+1}]$, for each $i \in \{0,\ldots,N-1\}$. These subintervals
will have length $h = 1/N$ and we can define our discretisation as follows:

\[
    D^h = \bigcup_{i=0}^{N - 1}I_i
\]

Upon which we can define a finite dimensional subspace of $H^1_0(D)$:

\begin{equation}
    \left(H_0^1(D)\right)^h =
        \{v \in H^1_0(D) : v \text{ is linear on } I_i,\ i \in \{0,\ldots,N-1\},
                        \  v \text{ is continuous on } [-1,1]\}
\end{equation}

Then we can choose the `hat functions' as our basis again which are defined
just as they were in \myref{eq:one-d-hat-basis} hence we can approximate $f(x)$
as follows:

\begin{equation}\label{eq:oned-stochastic-f-approx}
    f(x) \approx \sum_{j=0}^Nf_j\phi_i(x)
\end{equation}

where $f_j = f(x_j)$. This also allows us to write our approximate solution
process $u^h$ as follows:

\begin{equation}\label{eq:oned-stochastic-uh}
    u^h(x;\omega) = \sum_{j=0}^Nu_j(\omega)\phi_j(x)
\end{equation}

We now have a discrete representation for $u^h$ in the physical space but now
we need to turn our attention to approximating the diffusion coefficient
$\kappa(x;\omega)$ and the stochastic parts of the solution process
$u_j(\omega)$

\subsection{Polynomial Chaos}

Much like the physical case, constructing a finite dimensional approximation to
a probability space requires choosing a set of orthogonal functions to form a
basis which spans the space. As shown in \cite{gpc} members from the
\textit{Askey Scheme} of orthogonal polynomials can be used to construct finite
dimensional approximations to second order random processes.

By introducing the following notation for the expected value of a quantity:

\begin{equation}\label{eq:oned-stochastic-expect-notation}
    \expect{\cdot} = \int_{\Omega}(\cdot) \,dP
\end{equation}

allows us to express this orthogonality relation as follows:

\begin{equation}\label{oned-stochastic-orthog-relation}
    \expect{\chi_i\chi_j} = \expect{\chi_i}^2\delta_{ij}
\end{equation}

Depending on the probability measure of a space a different set of polynomials
are used. For example if the probability measure is for a
\textit{Gaussian process} then the \textit{Hermite polynomials} are used or in
the case of a \textit{Uniform process} the \textit{Legendre polynomials} are
used. A full table outlining the correspondence of probability measures to
polynomials can be found in \cite{general-poly-chaos}

Once the set of polynomials have been determined you can approximate a random
process as follows:

\begin{equation}
    \omega(\theta) = \sum_{s=1}^P\omega_s\chi_s(\theta)
\end{equation}

where $P = (d + p)!/d!p!$ and $d$ denotes the dimensionality of the
approximation and $p$ is the highest degree of polynomial used. For example in
the case of Legendre polynomials and $d=2$, $p=2$ that gives a total of 6
terms in the expansion. We can then define the following index scheme:

\begin{equation}
  \begin{array}{c c c }
    \alpha_1 = (0,0) & \alpha_2 = (1,0) & \alpha_3 = (0,1) \\
    \alpha_4 = (2,0) & \alpha_5 = (1,1) & \alpha_6 = (0,2)
  \end{array}
\end{equation}

Then the set of basis polynomials in this case would be given by:

\begin{equation}
  \begin{array}{l l l}
    \chi_{\alpha_1} = 1 & \chi_{\alpha_2} = \xi_1 & \chi_{\alpha_3} = \xi_2 \\
    \chi_{\alpha_4} = \frac{1}{2}(3\xi_1^2 - 1) &
    \chi_{\alpha_5} = \xi_1\xi_2 &
    \chi_{\alpha_6} = \frac{1}{2}(3\xi_2^2 - 1)
  \end{array}
\end{equation}

With an appropriate basis chosen we can now define a finite dimensional
subspace to the probability space $L^2(\Omega)$:

\begin{equation}
    \left(L^2(\Omega)\right)^P = span\{\chi_1, \ldots, \chi_P\}
\end{equation}

which allows us to rewrite our approximation to the solution process
\myref{eq:oned-stochastic-uh} as follows:

\begin{equation}\label{eq:oned-stochastic-uhp}
    u^{h,P}(x;\omega) = \sum_{j=0}^N\sum_{s=1}^Pu_{j,s}\chi_s(\xi)\phi_j(x)
\end{equation}

where $u^{h,P} \in V^{h,P} = \left(H^1_0(D)\right)^h \otimes
\left(L^2(\Omega)\right)^P$. In a similar manner we can also construct a finite
dimensional approximation to members of the test space $W$ as
$W^{h,P} = \left(H^1_0(D)\right)^h \otimes \left(L^2(\Omega)\right)^P$

\subsection{The Karhunen-Loeve Expansion}\label{sec:oned-stochastic-kl-expansion}

The Karhunen-Loeve (KL) expansion allows us to write any second order
stochastic process $X(\v{x},\omega)$ in the following way:

\begin{equation}
    X(\v{x}, \omega) = \bar{X}(\v{x})
    + \sum_{n = 0}^\infty\sqrt{\lambda_n}\beta_n(\v{x})\xi_n(\omega)
\end{equation}

where:

\begin{itemize}
    \item $\bar{X}(\v{x})$ denotes the expected value of the process
    \item $\{\xi_n(\omega)\}_{n=0}^\infty$ forms a set of uncorrelated random
          variables
    \item $\lambda_n$, $\beta_n(\v{x})$ denote the eigenvalues/eigenfunctions
          of the following eigenvalue problem:
          \[
                \int_DC(\v{x}_1, \v{x}_2)\beta(\v{x}_1)\, d\v{x}_1
                = \lambda\beta(\v{x}_2)
          \]
          where $C(\v{x}_1,\v{x}_2)$ denotes the correlation function of the
          random process.
\end{itemize}

The derivation of this representation is discussed in \cite{stochastic-fem} as
well as a number of its properties. One such property which is useful for our
purposes is that this representation is optimal in the sense that when we
truncate the series to a finite number of terms the mean squared error is
minimised.

Of course since this representation requires that we know the correlation
function of the random process so this will only be useful for expanding the
term $a(x;\omega)$ in \myref{eq:oned-stochastic}.  For the purposes of this
project we assume that $a(x;\omega)$ can be written as:

\begin{equation}
    a(x;\omega) = 1 + \epsilon\kappa(x;\omega)
\end{equation}

where $\epsilon < 1$ is a small parameter and  $\kappa(x;\omega)$ is
uniformly distributed taking values between $a$ and $b$ with mean $\mu$,
variance $\sigma^2$ with correlation function given by:

\begin{equation}
    C(x, y) = \sigma^2\exp\left(-\frac{|x - y|}{k}\right)
\end{equation}

where $k$ is the correlation length that for simplicity we set to $1$.
Therefore to determine $\beta_n, \lambda_n$ we have to solve the following
integral equation:

\begin{equation}\label{eq:oned-stochastic-kle-eigenvalue-problem}
    \sigma^2\int_{-1}^1\exp(-|x - y|)\beta_n(y)\, dy = \lambda_n\beta_n(x)
\end{equation}

A detailed discussion on finding the eigenvalues/eigenfunctions above can be
found in \cite{stochastic-fem} but solving this numerically will be sufficient
for our purposes. Therefore we will be able to approximate $a(x;\omega)$
as follows:

\begin{equation}\label{eq:oned-stochastic-kl-kappa}
    a(x;\omega) \approx 1 + \epsilon\left(
        \mu + \sigma^2\sum_{l=1}^d\sqrt{\lambda_l}\beta_l(x)\xi_l(\omega)
    \right)
\end{equation}

\subsubsection{Obtaining a Numeric Representation of the KL Expansion}

Unfortunately I was unable to find an existing method to numerically solve
\myref{eq:oned-stochastic-kle-eigenvalue-problem} using Python. Fortunately
this is a relatively easy problem to solve in Matlab with the use of the
\textit{Chebfun} numerical computation library \cite{chebfun}. The Matlab code
which solves this problem can be found in Listing \ref{code:matlab-eigen}.

\begin{lstlisting}[caption={Matlab code which finds the first 5 eigenvalues and
                            associated eigenfunctions},
                   label={code:matlab-eigen},
                   language=Matlab]
function KLEigs(k)
    K = @(x, y) exp(-abs(x-y));
    L = chebop(@(u) fred(K,u));
    [V,D] = eigs(L, k, 'lm');

    values = diag(D);
    dlmwrite('expansion-data.csv', transpose(values), ',');

    for i = 1:len(values)
        dlmwrite('expansion-data.csv', transpose(V{1,i}.points), '-append', 'delimiter', ',');
        dlmwrite('expansion-data.csv', transpose(V{1,i}.values), '-append', 'delimiter', ',');
    end
end
\end{lstlisting}

The results from running the above code with $k = 5$ can be found as part of
Appendix \ref{app:poly-chaos-code} in Table \ref{tab:eigenvalue-data}

In order to use these results from Matlab with our Python
code, we require a brief discussion of how the \textit{Chebfun} library
represents mathematical functions. As stated in \cite{chebfun-data}, the
fundamental data structure \incode{chebfun} is a numerical representation of a
function $f$ using a set of numbers $\{f_0, \ldots, f_N\}$ where $f_j = f(x_j)$
, $j \in \{0, \ldots, N\}$ where the $x_j$ are particular points in the domain
called Chebyshev points. The $f_j$ are then used with an interpolation method
called \textit{barycentric interpolation} which achieves a good, fast and stable
approximation to $f$.

Therefore in order to import the  Matlab results we simply need to
export the $f_j$, $x_j$ of each of the eigenfunctions we find. Then by using
the \incode{BarycentricInterpolator} which is part of the \textit{scipy} library
\cite{scipy} we can reconstruct the functions in our Python code. The code
which performs this reconstruction can be found in Listing
\ref{code:python-reconstruct-eigen}.

\begin{lstlisting}[caption={Python code which reconstructs the eigenfunctions
                            we found using Matlab},
                   label={code:python-reconstruct-eigen},
                   language=Python]
import csv
import scipy.interpolate import BarycentricInterpolator


def reconstruct_eigenfunctions(filename):
    """
    Given a filename, read the CSV data and reconstruct the eigenfunctions
    as they are represented by the Matlab code
    """

    # Read and parse the CSV data
    with open(filename) as f:
        reader = csv.reader(f)
        rows = [row for row in reader]

    # Eigenvalues are given by the first row of data
    eigenvalues = [float(l) for l in rows[0]]

    data = list()

    for i in range(len(eigenvalues)):

        # The remaining rows alternate between the x_j and f(x_j)
        # representing the i-th eigenfunction associated with the i-th eigenvalue
        p = BarycentricInterpolator([float(x) for x in rows[2*i + 1]],
                                    [float(y) for y in rows[2*i + 2]])

        data.append({'lambda': eigenvalues[i],
                     'beta': p})

    # Reverse the list so the eigenvalues are sorted from largest - smallest
    data.reverse()

    return data
\end{lstlisting}

Finally it is worth considering if this process does indeed give the right
result, thankfully as this particular correlation function
\myref{eq:oned-stochastic-kle-eigenvalue-problem} is discussed in
\cite{stochastic-fem} we can compare results. Hence as Figure
\ref{fig:kle-eigenfunctions} matches what we see in Figure $2.1$ in
\textit{Ghanem \& Spanos}, we can be confident that indeed we have obtained the
correct results.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{img/kle-eigenfunctions.pdf}
    \caption{The first 5 eigenfunctions and associated eigenvalues of
             \myref{eq:oned-stochastic-kle-eigenvalue-problem}, assuming unit
             variance}
    \label{fig:kle-eigenfunctions}
\end{figure}

\subsection{Derivation of Global System of Equations}

Since the weak formulation \myref{eq:wk-one-d-stochastic} has to be satisfied
$\forall w \in W$ it must in particular hold for the basis functions. So by
using the expansions \myref{eq:oned-stochastic-uhp},
\myref{eq:oned-stochastic-f-approx} and \myref{eq:oned-stochastic-kl-kappa} and
choosing $w = \phi_i(x)\chi_t(x)$ for each $i = \{0,\ldots,N\}$ and each $t =
\{1,\ldots,P\}$ and substituting into \myref{eq:wk-one-d-stochastic} we
obtain:

\begin{align}
  \begin{split}
    \int_{\Omega}\int_{-1}^1
    \left[1 + \epsilon\left(\mu +
        \sum_{l=1}^d\sqrt{\lambda_l}\beta_l(x)\xi_l(\omega)\right)\right]
      \frac{d}{dx}\left(\sum_{j=0}^N\sum_{s=1}^Pu_{s,j}\phi_j(x)\chi_s(\omega)\right)
      \phi_i'(x)\chi_t(\omega)\, dx\, d\Omega \\ =
    \int_{\Omega}\int_{-1}^1
      \left(\sum_{j=0}^Nf_j\phi_j(x)\right)
      \phi_i(x)\chi_t(\omega)\, dx\, d\Omega
  \end{split}
\end{align}

Then by the linearity of the integral and differential operator and taking
advantage of our notation for the expectation
\myref{eq:oned-stochastic-expect-notation} we may write this as:

\begin{align}\label{eq:oned-stochastic-discrete}
  \begin{split}
      \sum_{j=0}^N\sum_{s=1}^Pu_{s,j}\left[
          (1 + \epsilon\mu)\expect{\chi_s\chi_t}
          \int_{-1}^1\phi_j'(x)\phi_i'(x)\, dx +
          \epsilon\sum_{l=1}^d\sqrt{\lambda_l}\expect{\xi_l\chi_s\chi_t}
          \left(\int_{-1}^1 \beta_l(x)\phi_j'(x)\phi_i'(x)\, dx\right)
      \right]\\ =
      \expect{\chi_t}\sum_{j=0}^Nf_j
          \left(\int_{-1}^1\phi_j(x)\phi_i(x)\, dx\right)
  \end{split}
\end{align}

for each $j \in \{0,\ldots,N\}$ and each $t \in \{1,\ldots,P\}$. Just as in the
deterministic case, we know the solution at the endpoints due to the boundary
conditions on the problem \myref{eq:oned-stochastic} hence we can remove the
terms associated with $j = 0$ and $j = N$ from the system of equations. Then by
defining the $i$-th, $j$-th component of the matrix $A_{s,t}$ to be the terms
in the square brackets of \myref{eq:oned-stochastic-discrete} and the $i$-th,
$j$-th component of the matrix $M_{t}$ to be the bracketed terms on the right
hand side, we obtain the following:

\begin{equation}
    \sum_{j=1}^{N - 1}\sum_{s=1}^P(A_{s,t})_{i,j}u_{s,j} =
    \sum_{j=0}^N(M_t)_{i,j}f_j
\end{equation}

for $i \in \{1,\ldots,N - 1\}$ and $t \in \{1,\ldots,P\}$. This defines a
$(N - 1)P \times (N - 1)P$ system of linear equations $A\v{u} = M\v{f}$ in
which the global stiffness matrix $A$ takes the following block form:

\begin{equation}
    A = \left[\begin{array}{c c c}
            A_{1,1} & \cdots & A_{1,P} \\
            \vdots & & \vdots \\
            A_{P,1} & \cdots & A_{P,P}
        \end{array}\right]
\end{equation}

where $A_{s,t}$ are $(N - 1) \times (N - 1)$ matrices. Similarly the
global mass matrix $M$ takes the block form:

\begin{equation}
    M = \left[\begin{array}{c}
            M_1 \\
            \vdots \\
            M_P
        \end{array}\right]
\end{equation}

where $M_t$ are $(N - 1) \times (N + 1) $ matrices.

\section{Constructing the Global System}

In order to construct the global system of equations we need to determine the
form of each of the smaller matrices $A_{s,t}$ and $M_{s,t}$

\subsection{The Global Stiffness Matrix}

\subsubsection{Matrices on the Diagonal}

By setting $s=t$ in \myref{eq:oned-stochastic-discrete} we obtain the following
expression for the entries of the matrices on the diagonal of the
global stiffness matrix:

\begin{equation}
    A_{s,s} = \sum_{j=1}^{N-1}\left((1 + \epsilon\mu)\expect{\chi_s^2}
        \int_{-1}^1 \phi_j'(x)\phi_i'(x)\, dx
       + \epsilon\sigma^2\sum_{l=1}^d\sqrt{\lambda_n}\expect{\xi_l\chi_s^2}
       \int_{-1}^1 \beta_l(x)\phi_j'(x)\phi_i'(x) \, dx\right)
\end{equation}

for each $i \in \{1,\ldots,N-1\}$. Now if we consider the quantity
$\expect{\xi_l\chi_s^2}$ for a moment as the function $\chi_s^2$ is even and
$\xi_l$ is an odd function their product is odd therefore, as we are integrating
over a symmetric domain, this quantity vanishes. Hence the above reduces to:

\begin{equation}
    A_{s,s} = (1 + \epsilon\mu)\expect{\chi_s^2}\sum_{j=1}^{N - 1}\left(\int_{-1}^1
                \phi_j'(x)\phi_i'(x)\, dx\right)
\end{equation}

for each $i \in \{1,\ldots,N-1\}$. As the quantity $\mu\expect{\chi_s^2}$ is
fully deterministic and scalar we can treat this in a similar manner to the
deterministic case in  Chapter \ref{chap:oned-deterministic}. By setting $a =
(1 +\epsilon\mu)\expect{\chi_s^2}, b = 0, c = 0$ in
\myref{eq:oned-deterministic-discrete} and following a similar argument as
outlined in Section \ref{sec:oned-deterministic-local-stiffness} we obtain the
following form of the local stiffness matrix $A_{s,s}^{(k)}$:

\begin{equation}\label{eq:oned-stochasic-local-stifness-diag}
    A_{s,s}^{(k)} = \frac{(1 + \epsilon\mu)\expect{\chi_s^2}}{h_k}
              \left[\begin{array}{c c}
                1 & -1 \\ -1 & 1
              \end{array}\right]
\end{equation}

Continuing with the same argument as in Section
\ref{sec:oned-deterministic-global-stiffness-assembly} we find that the matrix
$A_{s,s}$ will have the form as shown in
\myref{eq:oned-deterministic-global-stiffness}.

\subsubsection{The Off-Diagonal Matrices}

Now if we consider the case where $s \neq t$ in
\myref{eq:oned-stochastic-discrete} we obtain:

\begin{equation}\label{eq:oned-stochastic-off-diagonal-stiffness}
    A_{s,t} = \sum_{j=1}^{N - 1}\left((1 + \epsilon)\mu\expect{\chi_s\chi_t}
        \int_{-1}^1 \phi_j'(x)\phi_i'(x)\, dx
       + \epsilon\sum_{l=1}^d\sqrt{\lambda_n}\expect{\xi_l\chi_s\chi_t}
       \int_{-1}^1 \beta_l(x)\phi_j'(x)\phi_i'(x)\, dx\right)
\end{equation}

for each $i \in \{1,\ldots,N-1\}$. By the orthogonality relation
$\expect{\chi_s\chi_t} = \expect{\chi_s^2}\delta_{st}$, the first term vanishes
and we are left with:

\begin{equation}
    A_{s,t} = \sum_{j=1}^{N - 1}\left(
        \epsilon\sum_{l=1}^d\sqrt{\lambda_l}\expect{\xi_l\chi_s\chi_t}
            \int_{-1}^1\beta_l(x)\phi_j'(x)\phi_i'(x)\, dx\right)
\end{equation}

Determining the entries to these off diagonal matrices can be done in a similar
manner to the diagonal matrices but some more care must be taken at each step
as we now longer have a simple scalar coefficient. Each of these off diagonal
matrices are comprised of $d$ matrices which are summed together. Let's for a
moment just consider one of these:

\begin{equation}
    \sum_{j=1}^{N - 1}\epsilon\left(\sqrt{\lambda_l}\expect{\xi_l\chi_s\chi_t}
        \int_{-1}^1\beta_l(x)\phi_j'(x)\phi_i'(x)\, dx\right)
\end{equation}

The quantity $\epsilon\sqrt{\lambda_l}\expect{\xi_l\chi_s\chi_t}$ is
deterministic and a scalar which we can easily evaluate, so focusing on the
integral, we know from Section \ref{sec:oned-deterministic-global-construct}
that we can rewrite each of the global basis functions $\phi_i,\phi_j$ locally
in each subinterval in terms of \myref{eq:oned-deterministic-psi-1} and
\myref{eq:oned-deterministic-psi-2}, so for the off diagonal matrices the local
stiffness matrix is given by:

\begin{equation}
    A^{(k)}_{m,n} = \int_{x_k}^{x_{k+1}}
        \beta_l(x)\psi_{k,m}'(x)\psi_{k,n}'(x)\, dx
\end{equation}

for each $m, n \in \{1, 2\}$. An important distinction to note here is that in
the off diagonal matrices with the introduction of the eigenfunctions
$\beta_l(x)$ the local sitffness matrix now changes depending on which
subinterval in the domain we are considering. With the local stiffness matrix
determined we can assemble the off diagonal matrix as seen in
\myref{eq:oned-deterministic-global-stiffness}

\subsection{The Global Mass Matrix}

As show in \ref{eq:oned-stochastic-discrete} the $t$-th block matrix in the
global mass matrix is given by:
\begin{equation}
    M_t = \expect{\chi_t}\sum_{j=0}^Nf_j\left(
        \int_{-1}^1\phi_j(x)\phi_i(x)\, dx\right)
\end{equation}

for each $i \in \{1,\ldots,N-1\}$. Since $\expect{\chi_t} =
\expect{\chi_t\chi_0} = \expect{\chi_0^2}\delta_{t, 0}$ by the orthogonality of
the stochastic basis vectors, the global mass matrix is given by:

\begin{equation}
    M = \left[\begin{array}{c}
        M_0 \\ \v{0} \\ \vdots \\ \v{0}
    \end{array}\right]
\end{equation}

and since $\expect{\chi_0^2} = 1$, $M_0$ is exactly the global mass matrix we
found in the deterministic problem \myref{eq:oned-deterministic-global-mass}.

\paragraph{Note:}

It is worth noting that the shape of the global mass matrix is due to the fact
in our problem we have a fully deterministic $f$. If we introduced some
uncertainty into $f$ then we would be in a similar situation to the left hand
side with a global block mass matrix with non zero entries down the diagonal.

\section{Example Problems and Results}


\subsection{Post Processing the Solution}

Now that we have our approximation to the solution solution process
$u^{h,P}(x,\omega)$ we can ask questions such as what is the mean of varience
of the process on the interval.

\subsubsection{Calculating the Mean}

Given the solution process $u^{h,P}$ we can calculate the mean by integrate it
over the probability space:

\begin{align}
  \begin{split}
    \mathbb{E}\left[u^{h,P}\right] = \expect{u^{h,P}} &=
    \int_\Omega\sum_{j=0}^N\sum_{s=1}^Pu_{j,s}\phi_j(x)\chi_s(\omega)\, d\Omega \\
    &= \sum_{j=0}^Nu_{j,s}\phi_j(x)\sum_{s=1}^P\int_\Omega\chi_s(\omega)\, d\Omega \\
    &= \sum_{j=0}^Nu_{j,0}\phi_j(x)
  \end{split}
\end{align}

This is due to the fact we can write the integral from above as follows:

\begin{equation}
    \int_\Omega\chi_s(\omega)\cdot 1\, d\Omega =
    \int_\Omega\chi_s(\omega)\chi_0(\omega)\, d\Omega = \delta_{s,0}
\end{equation}

by the orthogonality of the Legendre Polynomials.

\subsubsection{Calculating the Variance}

The variance of our approximation to the solution process is given by:

\begin{align}
  \begin{split}
    Var(u^{h,P}) &= \mathbb{E}\left[\left(u^{h,P}\right)^2\right]
                    - \left(\mathbb{E}\left[u^{h,P}\right]\right)^2 \\
      &= \int_\Omega\left(\sum_{j=1}^{N - 1}\sum_{s=1}^Pu_{j,s}\phi_j(x)\chi_s(\omega)\right)^2\, d\Omega
        - \left(\sum_{j=1}^{N - 1}u_{j,0}\phi_j(x)\right)^2 \\
      &= \int_\Omega\left(\sum_{i=1}^{N - 1}\sum_{j=1}^{N - 1}\sum_{s=1}^P\sum_{s=1}^P
           u_{j,s}u_{i,t}\phi_j(x)\phi_i(x)\chi_s(\omega)\chi_t(\omega)
         \right)\, d\Omega - \left(\sum_{j=1}^{N - 1}u_{j,0}\phi_j(x)\right)^2 \\
      &= \sum_{i=1}^{N - 1}\sum_{j=1}^{N - 1}\sum_{s=1}^P\sum_{t=0}^Pu_{j,s}u_{i,t}
            \phi_j(x)\phi_i(x)\int_\Omega\chi_s(\omega)\chi_t(\omega)\, d\Omega
            - \sum_{i=1}^{N - 1}\sum_{j=1}^{N - 1}u_{i,0}u_{j,0}\phi_j(x)\phi_i(x)
  \end{split}
\end{align}

Once again invoking the orthogonality relation for the stochastic basis vectors
$\chi_s$ the above reduces to:

\begin{align}
  \begin{split}
    Var(u^{h,P}) &= \sum_{i=1}^{N - 1}\sum_{j=1}^{N - 1}\sum_{s=1}^P
      u_{j,s}u_{i,s}\phi_j(x)\phi_i(x)\expect{\chi_s^2}
            - \sum_{i=1}^{N - 1}\sum_{j=1}^{N-1}u_{i,0}u_{j,0}\phi_j(x)\phi_i(x) \\
      &= \sum_{i=1}^{N-1}\sum_{j=1}^{N-1}\sum_{s=1}^P
            u_{j,s}u_{i,s}\phi_j(x)\phi_i(x)\expect{\chi_s^2}
  \end{split}
\end{align}

Now if we consider the support of the spatial basis functions as we can see in
Figure \ref{fig:oned-local-basis} for each $j$ the only non zero terms will be
when $|j - i| \leq 1$ so the variance is given by:

\begin{equation}
    Var(u^{h,P})= \sum_{j=1}^{N-1}\sum_{i=-1}^1\sum_{s=1}^P
      u_{j,s}u_{(j+i),s}\phi_j(x)\phi_{j+i}(x)\expect{\chi_s^2}
\end{equation}
